{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "> Things about NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual  Data Obtainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import logging\n",
    "from ipywidgets import interact, interact_manual, FileUpload\n",
    "from pathlib import Path\n",
    "from forgebox.html import DOM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unpackai.cosine import CosineSearch\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a text data management class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Textual:\n",
    "    \"\"\"\n",
    "    Obtain and manage textual data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str):\n",
    "        self.text = text.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"\"\"Text ({len(self.text)} chars), textual(),\n",
    "    train_path, val_path = textual.create_train_val()\"\"\"\n",
    "\n",
    "    def __call__(self, page_size: int = 1000) -> None:\n",
    "        \"\"\"\n",
    "        Previewing the first 200(or less) pages\n",
    "        - page_size: character number each page\n",
    "        \"\"\"\n",
    "        logging.info(f\"Previewing the first 200 pages\")\n",
    "\n",
    "        @interact\n",
    "        def show_text(page=(0, min(len(self.text)//page_size-1, 200), 1)):\n",
    "            display(self.text[page*page_size: (page+1)*page_size])\n",
    "\n",
    "    @classmethod\n",
    "    def from_url(cls, url: str):\n",
    "        res = requests.get(url)\n",
    "        if res.status_code == 200:\n",
    "            res.encoding = 'utf-8'\n",
    "            headers = res.headers\n",
    "            if \"html\" in str(headers).lower():\n",
    "                text = BeautifulSoup(res.text).text\n",
    "            else:\n",
    "                text = res.text\n",
    "            obj = cls(text)\n",
    "            obj.path = \"./text_data.txt\"\n",
    "            with open(obj.path, \"w\") as f:\n",
    "                f.write(obj.text)\n",
    "            return obj\n",
    "        else:\n",
    "            raise ConnectionError(f\"Error downloading: {url}\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, path: Path):\n",
    "        \"\"\"\n",
    "        Load a textual object from system path\n",
    "        \"\"\"\n",
    "        if Path(path).exists() == False:\n",
    "            raise FileExistsError(f\"Can not find {path}\")\n",
    "        with open(path, ) as f:\n",
    "            obj = cls(f.read())\n",
    "            obj.path = path\n",
    "            return obj\n",
    "    \n",
    "    @classmethod\n",
    "    def from_upload(\n",
    "        cls,\n",
    "        path: Path = Path(\"./uploaded_file.txt\")\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load textual with interactive upload button\n",
    "        \"\"\"\n",
    "        DOM(\"üóÉ Please upload a text file ended in .txt\", \"h4\")()\n",
    "        my_manual = interact_manual.options(manual_name=\"Upload\")\n",
    "        @my_manual\n",
    "        def create_upload(btn_upload = FileUpload(description=\"Choose File\")):\n",
    "            text = list(btn_upload.values())[-1]['content'].decode()\n",
    "            with open(path, \"w\") as f:\n",
    "                f.write(text)\n",
    "            return path\n",
    "        def uploaded():\n",
    "            result = create_upload.widget.result\n",
    "            if result is None:\n",
    "                raise FileExistsError(\n",
    "                    \"You have to upload the txt file first\")\n",
    "            return cls.from_path(result)\n",
    "        return uploaded\n",
    "        \n",
    "\n",
    "    def create_train_val(\n",
    "            self,\n",
    "            valid_ratio=.2,\n",
    "            train_path=\"./train_text.txt\",\n",
    "            val_path=\"./val_text.txt\"):\n",
    "        \"\"\"\n",
    "        create 2 files:\n",
    "        - ./train_text.txt\n",
    "        - ./val_text.txt\n",
    "        \"\"\"\n",
    "        split = int(len(self.text)*(valid_ratio))\n",
    "        with open(train_path, \"w\") as f:\n",
    "            f.write(self.text[split:])\n",
    "        with open(val_path, \"w\") as f:\n",
    "            f.write(self.text[:split])\n",
    "        return train_path, val_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>üóÉ Please upload a text file ended in .txt</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd61fd3fca8424cbf0784b4b823f88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FileUpload(value={}, description='Choose File'), Button(description='Upload', style=Butt‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uploaded = Textual.from_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text (65792 chars), textual(),\n",
       "    train_path, val_path = textual.create_train_val()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textual = uploaded()\n",
    "textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b5aea34cfa42258bfeb067d963f2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=32, description='page', max=64), Output()), _dom_classes=('widget-intera‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "textual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED = \"albert-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f13593270bf4f639e84b7f14256bfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=684.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c80cc4996c948938cff446f48dd368a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=47376696.0, style=ProgressStyle(descrip‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d808fdc761364c939caed6b10dd478e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descripti‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69859ac0e6d483e807782d00dde47ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1312669.0, style=ProgressStyle(descript‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class InterpEmbeddings:\n",
    "    \"\"\"\n",
    "    interp = InterpEmbeddings(embedding_matrix, vocab_dict)\n",
    "    \n",
    "    interp.search(\"computer\")\n",
    "    \n",
    "    # visualize the embedding with tensorboard \n",
    "    interp.visualize_in_tb()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_matrix: np.ndarray,\n",
    "        vocab: Dict[int, str]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        embedding_matrix: np.ndarray, embedding matrix of shape:\n",
    "            (num_items, hidden_size)\n",
    "        \"\"\"\n",
    "        self.base = embedding_matrix\n",
    "        self.cosine = CosineSearch(embedding_matrix)\n",
    "        self.vocab = vocab\n",
    "        self.c2i = dict((v, k) for k, v in vocab.items())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        cls = self.__class__.__name__\n",
    "        return f\"{cls} with\\n\\t{self.cosine}\"\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        category: str,\n",
    "        top_k: int = 20,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        search for similar words with embedding and vocabulary dictionary\n",
    "        \"\"\"\n",
    "        token_id = self.c2i.get(category)\n",
    "        if token_id is None:\n",
    "            match_list = []\n",
    "            for token_id, token in self.vocab.items():\n",
    "                if category.lower() in str(token).lower():\n",
    "                    match_list.append({\"token\": token, \"token_id\": token_id})\n",
    "            if len(match_list)==0:\n",
    "                raise KeyError(\n",
    "                    f\"[UnpackAI] category: {category} not in vocabulary\")\n",
    "            else:\n",
    "                match_df = pd.DataFrame(match_list)\n",
    "                DOM(\"Search with the following categories\",\"h3\")()\n",
    "                display(match_df)\n",
    "                token_ids = list(match_df.token_id)\n",
    "        else:\n",
    "            DOM(f\"Search with token id {token_id}\",\"h3\")()\n",
    "            token_ids = [token_id,]\n",
    "\n",
    "        # combine multiple tokens into 1\n",
    "        vec = self.base[token_ids].mean(0)\n",
    "\n",
    "        # distance search\n",
    "        closest, similarity = self.cosine.search(vec, return_similarity=True)\n",
    "        \n",
    "        closest = closest[:top_k]\n",
    "        similarity = similarity[:top_k]\n",
    "        tokens = list(self.vocab.get(idx) for idx in closest)\n",
    "        return pd.DataFrame({\n",
    "            \"tokens\": tokens,\n",
    "            \"idx\": closest,\n",
    "            \"similarity\": similarity})\n",
    "    \n",
    "    def visualize_in_tb(\n",
    "        self,\n",
    "        log_dir:str=\"./logs\",\n",
    "        selection: np.ndarray=None,\n",
    "        first_k:int=500,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Visualize the embedding in tensorboard\n",
    "        For now this function is only supported on colab\n",
    "        \"\"\"\n",
    "        # since this won't be excute too many times within a notebook\n",
    "        # in large chances... so to avoid missing library when import\n",
    "        # other function under this module: we import related stuff here\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        # this version's pd has vc for quick value counts\n",
    "        from forgebox.imports import pd\n",
    "        import tensorflow as tf\n",
    "        import tensorboard as tb\n",
    "        import os\n",
    "        \n",
    "        # possible tensorflow version error\n",
    "        tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "        os.system(f\"rm -rf {log_dir}\")\n",
    "        writer = SummaryWriter(log_dir=log_dir,)\n",
    "        self.i2c = dict((v,k) for k,v in self.c2i.items())  \n",
    "        tokens = list(self.i2c.get(i) for i in range(len(self.i2c)))\n",
    "        \n",
    "        if selection is None:\n",
    "            vecs = self.base[:first_k]\n",
    "            tokens = tokens[:first_k]\n",
    "        else:\n",
    "            # select a pool of tokens for visualizaiton\n",
    "            tokens = tokens[selection][:first_k]\n",
    "            vecs = self.base[selection][:first_k]\n",
    "        writer.add_embedding(vecs, metadata=tokens,)\n",
    "        # prompts for next step\n",
    "        print(f\"Please run the the following command in a cell\")\n",
    "        print(\"%load_ext tensorboard\")\n",
    "        print(f\"%tensorboard  --logdir {log_dir}\")\n",
    "\n",
    "\n",
    "class InterpEmbeddingsTokenizer(InterpEmbeddings):\n",
    "    def __init__(self,\n",
    "                 embedding_matrix,\n",
    "                 tokenizer):\n",
    "        \"\"\"\n",
    "        embedding_matrix: np.ndarray, embedding matrix of shape:\n",
    "            (num_items, hidden_size)\n",
    "        tokenizer: a huggingface tokenizer\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            embedding_matrix,\n",
    "            dict((v, k) for k, v in tokenizer.vocab.items()))\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        word: str,\n",
    "        filter_special_token: bool = True,\n",
    "        top_k: int = 20,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        search for similar words with embedding and\n",
    "            tokenizer's encode/ decode\n",
    "        \"\"\"\n",
    "        token_ids = self.tokenizer.encode(word)\n",
    "        if filter_special_token:\n",
    "            token_ids = list(t for t in token_ids if t > 110)\n",
    "\n",
    "        # combine multiple tokens into 1\n",
    "        vec = self.base[token_ids].mean(0)\n",
    "\n",
    "        # distance search\n",
    "        closest, similarity = self.cosine.search(vec, return_similarity=True)\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(closest)\n",
    "        return pd.DataFrame({\n",
    "            \"tokens\": tokens,\n",
    "            \"idx\": closest,\n",
    "            \"similarity\": similarity}).head(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '[CLS]',\n",
       " 'eos_token': '[SEP]',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertEmbeddings(\n",
       "  (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 128)\n",
       "  (token_type_embeddings): Embedding(2, 128)\n",
       "  (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 128)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = model.embeddings.word_embeddings.weight.data.numpy()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.values>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding with tokenizer search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = InterpEmbeddingsTokenizer(\n",
    "    embedding_matrix,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>idx</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>‚ñÅwife</td>\n",
       "      <td>663</td>\n",
       "      <td>2.796573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>‚ñÅgirlfriend</td>\n",
       "      <td>5606</td>\n",
       "      <td>1.874535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>‚ñÅspouse</td>\n",
       "      <td>16663</td>\n",
       "      <td>1.867864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>‚ñÅwives</td>\n",
       "      <td>11333</td>\n",
       "      <td>1.867352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>‚ñÅhusband</td>\n",
       "      <td>1253</td>\n",
       "      <td>1.826611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>‚ñÅdaughter</td>\n",
       "      <td>783</td>\n",
       "      <td>1.810506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>wife</td>\n",
       "      <td>13611</td>\n",
       "      <td>1.751927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>‚ñÅwidow</td>\n",
       "      <td>5151</td>\n",
       "      <td>1.605985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>‚ñÅfiancee</td>\n",
       "      <td>22947</td>\n",
       "      <td>1.557384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>‚ñÅsister</td>\n",
       "      <td>1035</td>\n",
       "      <td>1.515947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>‚ñÅdaughters</td>\n",
       "      <td>4909</td>\n",
       "      <td>1.495724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>‚ñÅwoman</td>\n",
       "      <td>524</td>\n",
       "      <td>1.459538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>‚ñÅson</td>\n",
       "      <td>433</td>\n",
       "      <td>1.451740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>‚ñÅmarried</td>\n",
       "      <td>567</td>\n",
       "      <td>1.424582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>‚ñÅfiance</td>\n",
       "      <td>22324</td>\n",
       "      <td>1.421876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>‚ñÅgranddaughter</td>\n",
       "      <td>15176</td>\n",
       "      <td>1.416612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>‚ñÅbride</td>\n",
       "      <td>8034</td>\n",
       "      <td>1.412241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>‚ñÅmother</td>\n",
       "      <td>449</td>\n",
       "      <td>1.410529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>‚ñÅmistress</td>\n",
       "      <td>10427</td>\n",
       "      <td>1.394689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>‚ñÅpartner</td>\n",
       "      <td>2417</td>\n",
       "      <td>1.379885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tokens    idx  similarity\n",
       "0            ‚ñÅwife    663    2.796573\n",
       "1      ‚ñÅgirlfriend   5606    1.874535\n",
       "2          ‚ñÅspouse  16663    1.867864\n",
       "3           ‚ñÅwives  11333    1.867352\n",
       "4         ‚ñÅhusband   1253    1.826611\n",
       "5        ‚ñÅdaughter    783    1.810506\n",
       "6             wife  13611    1.751927\n",
       "7           ‚ñÅwidow   5151    1.605985\n",
       "8         ‚ñÅfiancee  22947    1.557384\n",
       "9          ‚ñÅsister   1035    1.515947\n",
       "10      ‚ñÅdaughters   4909    1.495724\n",
       "11          ‚ñÅwoman    524    1.459538\n",
       "12            ‚ñÅson    433    1.451740\n",
       "13        ‚ñÅmarried    567    1.424582\n",
       "14         ‚ñÅfiance  22324    1.421876\n",
       "15  ‚ñÅgranddaughter  15176    1.416612\n",
       "16          ‚ñÅbride   8034    1.412241\n",
       "17         ‚ñÅmother    449    1.410529\n",
       "18       ‚ñÅmistress  10427    1.394689\n",
       "19        ‚ñÅpartner   2417    1.379885"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp.search(\"wife\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding with just an token id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITEMS = 500\n",
    "\n",
    "# an embedding maxtrix, in shape of\n",
    "movie_embedding = np.random.rand(NUM_ITEMS,42)\n",
    "\n",
    "# a dictionary mapping index to string\n",
    "vocab = dict((i, f\"movie #{i}\") for i in range(NUM_ITEMS,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the example embedding and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = InterpEmbeddings(movie_embedding, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Search with token id 22</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>idx</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>movie #22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.287944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>movie #74</td>\n",
       "      <td>74</td>\n",
       "      <td>0.240893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>movie #346</td>\n",
       "      <td>346</td>\n",
       "      <td>0.238294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>movie #239</td>\n",
       "      <td>239</td>\n",
       "      <td>0.238032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>movie #126</td>\n",
       "      <td>126</td>\n",
       "      <td>0.236948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>movie #414</td>\n",
       "      <td>414</td>\n",
       "      <td>0.236702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>movie #457</td>\n",
       "      <td>457</td>\n",
       "      <td>0.235410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>movie #458</td>\n",
       "      <td>458</td>\n",
       "      <td>0.234861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>movie #256</td>\n",
       "      <td>256</td>\n",
       "      <td>0.233834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>movie #138</td>\n",
       "      <td>138</td>\n",
       "      <td>0.233646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>movie #376</td>\n",
       "      <td>376</td>\n",
       "      <td>0.233056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>movie #311</td>\n",
       "      <td>311</td>\n",
       "      <td>0.231898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>movie #321</td>\n",
       "      <td>321</td>\n",
       "      <td>0.231871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>movie #103</td>\n",
       "      <td>103</td>\n",
       "      <td>0.231282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>movie #323</td>\n",
       "      <td>323</td>\n",
       "      <td>0.231186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>movie #198</td>\n",
       "      <td>198</td>\n",
       "      <td>0.231019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>movie #258</td>\n",
       "      <td>258</td>\n",
       "      <td>0.230791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>movie #408</td>\n",
       "      <td>408</td>\n",
       "      <td>0.230484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>movie #464</td>\n",
       "      <td>464</td>\n",
       "      <td>0.230410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>movie #159</td>\n",
       "      <td>159</td>\n",
       "      <td>0.230120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tokens  idx  similarity\n",
       "0    movie #22   22    0.287944\n",
       "1    movie #74   74    0.240893\n",
       "2   movie #346  346    0.238294\n",
       "3   movie #239  239    0.238032\n",
       "4   movie #126  126    0.236948\n",
       "5   movie #414  414    0.236702\n",
       "6   movie #457  457    0.235410\n",
       "7   movie #458  458    0.234861\n",
       "8   movie #256  256    0.233834\n",
       "9   movie #138  138    0.233646\n",
       "10  movie #376  376    0.233056\n",
       "11  movie #311  311    0.231898\n",
       "12  movie #321  321    0.231871\n",
       "13  movie #103  103    0.231282\n",
       "14  movie #323  323    0.231186\n",
       "15  movie #198  198    0.231019\n",
       "16  movie #258  258    0.230791\n",
       "17  movie #408  408    0.230484\n",
       "18  movie #464  464    0.230410\n",
       "19  movie #159  159    0.230120"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp.search(\"movie #22\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
